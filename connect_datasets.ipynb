{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bokar\\Documents\\GitHub\\itmo-ai-stream\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обработано 0 строк\n",
      "Обработано 100 строк\n",
      "Обработано 200 строк\n",
      "Обработано 300 строк\n",
      "Обработано 400 строк\n",
      "Обработано 500 строк\n",
      "Обработано 600 строк\n",
      "Обработано 700 строк\n",
      "Обработано 800 строк\n",
      "Обработано 900 строк\n",
      "Обработано 1000 строк\n",
      "Обработано 1100 строк\n",
      "Обработано 1200 строк\n",
      "Обработано 1300 строк\n",
      "Обработано 1400 строк\n",
      "Обработано 1500 строк\n",
      "Обработано 1600 строк\n",
      "Обработано 1700 строк\n",
      "Обработано 1800 строк\n",
      "Обработано 1900 строк\n",
      "Обработано 2000 строк\n",
      "Обработано 2100 строк\n",
      "Обработано 2200 строк\n",
      "Обработано 2300 строк\n",
      "Обработано 2400 строк\n",
      "Обработано 2500 строк\n",
      "Обработано 2600 строк\n",
      "Обработано 2700 строк\n",
      "Обработано 2800 строк\n",
      "Обработано 2900 строк\n",
      "Обработано 3000 строк\n",
      "Обработано 3100 строк\n",
      "Обработано 3200 строк\n",
      "Обработано 3300 строк\n",
      "Обработано 3400 строк\n",
      "Обработано 3500 строк\n",
      "Обработано 3600 строк\n",
      "Обработано 3700 строк\n",
      "Обработано 3800 строк\n",
      "Обработано 3900 строк\n",
      "Обработано 4000 строк\n",
      "Обработано 4100 строк\n",
      "Обработано 4200 строк\n",
      "Обработано 4300 строк\n",
      "Обработано 4400 строк\n",
      "Обработано 4500 строк\n",
      "Обработано 4600 строк\n",
      "Обработано 4700 строк\n",
      "Обработано 4800 строк\n",
      "Обработано 4900 строк\n",
      "Обработано 5000 строк\n",
      "Обработано 5100 строк\n",
      "Обработано 5200 строк\n",
      "Обработано 5300 строк\n",
      "Обработано 5400 строк\n",
      "Обработано 5500 строк\n",
      "Обработано 5600 строк\n",
      "Обработано 5700 строк\n",
      "Обработано 5800 строк\n",
      "Обработано 5900 строк\n",
      "Обработано 6000 строк\n",
      "Обработано 6100 строк\n",
      "Обработано 6200 строк\n",
      "Обработано 6300 строк\n",
      "Обработано 6400 строк\n",
      "Обработано 6500 строк\n",
      "Обработано 6600 строк\n",
      "Обработано 6700 строк\n",
      "Обработано 6800 строк\n",
      "Обработано 6900 строк\n",
      "Обработано 7000 строк\n",
      "Обработано 7100 строк\n",
      "Обработано 7200 строк\n",
      "Обработано 7300 строк\n",
      "Обработано 7400 строк\n",
      "Обработано 7500 строк\n",
      "Обработано 7600 строк\n",
      "Обработано 7700 строк\n",
      "Обработано 7800 строк\n",
      "Обработано 7900 строк\n",
      "Обработано 8000 строк\n",
      "Обработано 8100 строк\n",
      "Обработано 8200 строк\n",
      "Обработано 8300 строк\n",
      "Обработано 8400 строк\n",
      "Обработано 8500 строк\n",
      "Обработано 8600 строк\n",
      "Обработано 8700 строк\n",
      "Обработано 8800 строк\n",
      "Обработано 8900 строк\n",
      "Обработано 9000 строк\n",
      "Обработано 9100 строк\n",
      "Обработано 9200 строк\n",
      "Обработано 9300 строк\n",
      "Обработано 9400 строк\n",
      "Обработано 9500 строк\n",
      "Обработано 9600 строк\n",
      "Обработано 9700 строк\n",
      "Обработано 9800 строк\n",
      "Обработано 9900 строк\n",
      "Обработано 10000 строк\n",
      "Обработано 10100 строк\n",
      "Обработано 10200 строк\n",
      "Обработано 10300 строк\n",
      "Обработано 10400 строк\n",
      "Обработано 10500 строк\n",
      "Обработано 10600 строк\n",
      "Обработано 10700 строк\n",
      "Обработано 10800 строк\n",
      "Обработано 10900 строк\n",
      "Обработано 11000 строк\n",
      "Обработано 11100 строк\n",
      "Обработано 11200 строк\n",
      "Обработано 11300 строк\n",
      "Обработано 11400 строк\n",
      "Обработано 11500 строк\n",
      "Обработано 11600 строк\n",
      "Обработано 11700 строк\n",
      "Обработано 11800 строк\n",
      "Обработано 11900 строк\n",
      "Обработано 12000 строк\n",
      "Обработано 12100 строк\n",
      "Обработано 12200 строк\n",
      "Обработано 12300 строк\n",
      "Обработано 12400 строк\n",
      "Обработано 12500 строк\n",
      "Обработано 12600 строк\n",
      "Обработано 12700 строк\n",
      "Обработано 12800 строк\n",
      "Обработано 12900 строк\n",
      "Обработано 13000 строк\n",
      "Обработано 13100 строк\n",
      "Обработано 13200 строк\n",
      "Обработано 13300 строк\n",
      "Обработано 13400 строк\n",
      "Обработано 13500 строк\n",
      "Обработано 13600 строк\n",
      "Обработано 13700 строк\n",
      "Обработано 13800 строк\n",
      "Обработано 13900 строк\n",
      "Обработано 14000 строк\n",
      "Обработано 14100 строк\n",
      "Обработано 14200 строк\n",
      "Обработано 14300 строк\n",
      "Обработано 14400 строк\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['severe_toxic', 'identity_hate'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 82\u001b[0m\n\u001b[0;32m     79\u001b[0m df4[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(combined_df), \u001b[38;5;28mlen\u001b[39m(combined_df) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(df4))  \u001b[38;5;66;03m# Создание уникальных ID\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# Выбор нужных столбцов\u001b[39;00m\n\u001b[1;32m---> 82\u001b[0m df4 \u001b[38;5;241m=\u001b[39m \u001b[43mdf4\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcomment_text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtoxic\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msevere_toxic\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mobscene\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mthreat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minsult\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43midentity_hate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m# Объединение с уже существующим объединенным датасетом\u001b[39;00m\n\u001b[0;32m     85\u001b[0m final_combined_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([combined_df, df4], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\bokar\\Documents\\GitHub\\itmo-ai-stream\\myenv\\Lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\bokar\\Documents\\GitHub\\itmo-ai-stream\\myenv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bokar\\Documents\\GitHub\\itmo-ai-stream\\myenv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['severe_toxic', 'identity_hate'] not in index\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from detoxify import Detoxify\n",
    "\n",
    "# Функция для анализа токсичности с помощью Detoxify\n",
    "def analyze_toxicity(text):\n",
    "    try:\n",
    "        # Загрузка модели Detoxify\n",
    "        model = Detoxify('original')  # Используем предобученную модель\n",
    "        results = model.predict(text)  # Анализ текста\n",
    "        \n",
    "        # Преобразуем значения в бинарные (1 или 0) в зависимости от порога 0.65\n",
    "        binary_results = {key: 1 if value > 0.65 else 0 for key, value in results.items()}\n",
    "        \n",
    "        return binary_results\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при анализе текста: {e}\")\n",
    "        return {\n",
    "            'toxic': 0,\n",
    "            'severe_toxic': 0,\n",
    "            'obscene': 0,\n",
    "            'threat': 0,\n",
    "            'insult': 0,\n",
    "            'identity_hate': 0\n",
    "        }\n",
    "\n",
    "# Загрузка данных\n",
    "df1 = pd.read_csv('jigsaw-toxic-comment-train.csv')\n",
    "df2 = pd.read_csv('jigsaw-toxic-comment-train-google-ru-cleaned.csv')\n",
    "df3 = pd.read_csv('jigsaw-unintended-bias-train_ru_clean.csv')\n",
    "df4 = pd.read_csv('russian-language-toxic-comments.csv')\n",
    "\n",
    "# Приведение столбцов к единому формату\n",
    "# Для первого датасета\n",
    "df1['toxic'] = df1[['severe_toxicity', 'obscene', 'threat', 'insult', 'identity_attack']].max(axis=1)\n",
    "df1['severe_toxic'] = df1['severe_toxicity']\n",
    "df1['obscene'] = df1['obscene']\n",
    "df1['threat'] = df1['threat']\n",
    "df1['insult'] = df1['insult']\n",
    "df1['identity_hate'] = df1['identity_attack']\n",
    "\n",
    "# Для второго датасета\n",
    "df2['toxic'] = df2['toxic']\n",
    "df2['severe_toxic'] = df2['severe_toxic']\n",
    "df2['obscene'] = df2['obscene']\n",
    "df2['threat'] = df2['threat']\n",
    "df2['insult'] = df2['insult']\n",
    "df2['identity_hate'] = df2['identity_hate']\n",
    "\n",
    "# Для третьего датасета\n",
    "df3['toxic'] = df3[['severe_toxicity', 'obscene', 'threat', 'insult', 'identity_attack']].max(axis=1)\n",
    "df3['severe_toxic'] = df3['severe_toxicity']\n",
    "df3['obscene'] = df3['obscene']\n",
    "df3['threat'] = df3['threat']\n",
    "df3['insult'] = df3['insult']\n",
    "df3['identity_hate'] = df3['identity_attack']\n",
    "\n",
    "# Выбор нужных столбцов\n",
    "df1 = df1[['id', 'comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]\n",
    "df2 = df2[['id', 'comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]\n",
    "df3 = df3[['id', 'comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]\n",
    "\n",
    "# Объединение первых трех датасетов\n",
    "combined_df = pd.concat([df1, df2, df3], ignore_index=True)\n",
    "\n",
    "# Обработка пропущенных значений (если есть)\n",
    "combined_df.fillna(0, inplace=True)\n",
    "\n",
    "# Добавление новых колонок к четвертому датасету с помощью Detoxify\n",
    "for index, row in df4.iterrows():\n",
    "    if index % 100 == 0:\n",
    "        print(f\"Обработано {index} строк\")\n",
    "    toxicity_scores = analyze_toxicity(row['comment'])\n",
    "    for key, value in toxicity_scores.items():\n",
    "        df4.at[index, key] = value\n",
    "\n",
    "# Переименование столбцов для соответствия формату\n",
    "df4.rename(columns={'comment': 'comment_text'}, inplace=True)\n",
    "df4['id'] = range(len(combined_df), len(combined_df) + len(df4))  # Создание уникальных ID\n",
    "\n",
    "# Выбор нужных столбцов\n",
    "df4 = df4[['id', 'comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]\n",
    "\n",
    "# Объединение с уже существующим объединенным датасетом\n",
    "final_combined_df = pd.concat([combined_df, df4], ignore_index=True)\n",
    "\n",
    "# Обработка пропущенных значений (если есть)\n",
    "final_combined_df.fillna(0, inplace=True)\n",
    "\n",
    "# Сохранение в CSV\n",
    "final_combined_df.to_csv('final_combined_toxicity_data.csv', index=False)\n",
    "\n",
    "print(\"Объединение завершено. Данные сохранены в 'final_combined_toxicity_data.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install python-dotenv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

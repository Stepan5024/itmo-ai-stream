{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bokar\\AppData\\Local\\Temp\\ipykernel_672\\1145031837.py:2: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('final_combined_toxicity_data.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Преобразование завершено. Данные сохранены в 'final_combined_toxicity_data_updated.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Загрузка данных\n",
    "df = pd.read_csv('final_combined_toxicity_data.csv')\n",
    "\n",
    "# Применение правила: если значение > 0.65, то 1, иначе 0\n",
    "# Выбираем только нужные столбцы для преобразования\n",
    "toxicity_columns = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "# Применяем правило к каждому столбцу\n",
    "for column in toxicity_columns:\n",
    "    df[column] = df[column].apply(lambda x: 1 if x > 0.65 else 0)\n",
    "\n",
    "# Сохраним обновленный DataFrame в новый файл\n",
    "df.to_csv('final_combined_toxicity_data_updated.csv', index=False)\n",
    "\n",
    "print(\"Преобразование завершено. Данные сохранены в 'final_combined_toxicity_data_updated.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bokar\\AppData\\Local\\Temp\\ipykernel_672\\1581304433.py:4: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('final_combined_toxicity_data_updated.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Первые 5 строк данных:\n",
      "      id                                       comment_text  toxic  \\\n",
      "0  59848  This is so cool. It's like, 'would you want yo...      0   \n",
      "1  59849  Thank you!! This would make my life a lot less...      0   \n",
      "2  59852  This is such an urgent design problem; kudos t...      0   \n",
      "3  59855  Is this something I'll be able to install on m...      0   \n",
      "4  59856               haha you guys are a bunch of losers.      1   \n",
      "\n",
      "   severe_toxic  obscene  threat  insult  identity_hate  \n",
      "0             0        0       0       0              0  \n",
      "1             0        0       0       0              0  \n",
      "2             0        0       0       0              0  \n",
      "3             0        0       0       0              0  \n",
      "4             0        0       0       1              0  \n",
      "\n",
      "Размерность данных (строки, столбцы):\n",
      "(3942562, 8)\n",
      "\n",
      "Информация о данных:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3942562 entries, 0 to 3942561\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Dtype \n",
      "---  ------         ----- \n",
      " 0   id             object\n",
      " 1   comment_text   object\n",
      " 2   toxic          int64 \n",
      " 3   severe_toxic   int64 \n",
      " 4   obscene        int64 \n",
      " 5   threat         int64 \n",
      " 6   insult         int64 \n",
      " 7   identity_hate  int64 \n",
      "dtypes: int64(6), object(2)\n",
      "memory usage: 240.6+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Загрузка данных\n",
    "df = pd.read_csv('final_combined_toxicity_data_updated.csv')\n",
    "\n",
    "# 1. Просмотр первых нескольких строк данных\n",
    "print(\"Первые 5 строк данных:\")\n",
    "print(df.head())\n",
    "\n",
    "# 2. Проверка размерности данных\n",
    "print(\"\\nРазмерность данных (строки, столбцы):\")\n",
    "print(df.shape)\n",
    "\n",
    "# 3. Типы данных и наличие пропущенных значений\n",
    "print(\"\\nИнформация о данных:\")\n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Найдено дубликатов по comment_text: 80443\n",
      "Данные после удаления дубликатов по comment_text: 3886577 строк\n"
     ]
    }
   ],
   "source": [
    "# Проверка на дубликаты по столбцу comment_text\n",
    "duplicates_comment_text = df[df.duplicated('comment_text', keep=False)]\n",
    "print(f\"Найдено дубликатов по comment_text: {len(duplicates_comment_text)}\")\n",
    "\n",
    "# Удаление дубликатов по столбцу comment_text\n",
    "df = df.drop_duplicates(subset='comment_text', keep='first')\n",
    "print(f\"Данные после удаления дубликатов по comment_text: {len(df)} строк\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Первые 5 строк данных после обработки:\n",
      "      id                                       comment_text  toxic  \\\n",
      "0  59848  this is so cool its like would you want your m...      0   \n",
      "1  59849  thank youии this would make my life a lot less...      0   \n",
      "2  59852  this is such an urgent design problem kudos to...      0   \n",
      "3  59855  is this something ill be able to install on my...      0   \n",
      "4  59856                haha you guys are a bunch of losers      1   \n",
      "\n",
      "   severe_toxic  obscene  threat  insult  identity_hate  \n",
      "0             0        0       0       0              0  \n",
      "1             0        0       0       0              0  \n",
      "2             0        0       0       0              0  \n",
      "3             0        0       0       0              0  \n",
      "4             0        0       0       1              0  \n",
      "\n",
      "Размерность данных (строки, столбцы):\n",
      "(3886577, 8)\n",
      "\n",
      "Информация о данных:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3886577 entries, 0 to 3942561\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Dtype \n",
      "---  ------         ----- \n",
      " 0   id             object\n",
      " 1   comment_text   object\n",
      " 2   toxic          int64 \n",
      " 3   severe_toxic   int64 \n",
      " 4   obscene        int64 \n",
      " 5   threat         int64 \n",
      " 6   insult         int64 \n",
      " 7   identity_hate  int64 \n",
      "dtypes: int64(6), object(2)\n",
      "memory usage: 266.9+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Словарь для замены символов\n",
    "replacements = {\n",
    "    \"1\": \"и\", \"!\": \"и\", \"|\": \"и\", \"0\": \"о\", \"@\": \"а\", \"$\": \"с\", \"#\": \"х\", \"%\": \"п\",\n",
    "    \"&\": \"и\", \"*\": \"\", \"(\": \"\", \")\": \"\", \"-\": \"\", \"_\": \"\", \".\": \"\", \",\": \"\"\n",
    "}\n",
    "\n",
    "# Функция для замены символов\n",
    "def replace_special_chars(text):\n",
    "    for char, replacement in replacements.items():\n",
    "        text = text.replace(char, replacement)\n",
    "    return text\n",
    "\n",
    "# Функция для удаления лишних символов и смайликов\n",
    "def remove_extra_chars(text):\n",
    "    # Удаляем смайлики и лишние символы\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Удаляем всё, кроме букв, цифр и пробелов\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # Удаляем лишние пробелы\n",
    "    return text.strip()\n",
    "\n",
    "# Функция для нормализации текста\n",
    "def normalize_text(text):\n",
    "    # Приводим к нижнему регистру\n",
    "    text = text.lower()\n",
    "    # Заменяем спецсимволы\n",
    "    text = replace_special_chars(text)\n",
    "    # Удаляем лишние символы и смайлики\n",
    "    text = remove_extra_chars(text)\n",
    "    return text\n",
    "\n",
    "# Основная функция для обработки текста\n",
    "def preprocess_text(text):\n",
    "    # Нормализация текста\n",
    "    text = normalize_text(text)\n",
    "    return text  # Возвращаем обработанный текст\n",
    "\n",
    "# Применяем обработку текста и заменяем исходный столбец\n",
    "df[\"comment_text\"] = df[\"comment_text\"].apply(preprocess_text)\n",
    "\n",
    "# Вывод результата\n",
    "print(\"Первые 5 строк данных после обработки:\")\n",
    "print(df.head())\n",
    "\n",
    "# Проверка размерности данных\n",
    "print(\"\\nРазмерность данных (строки, столбцы):\")\n",
    "print(df.shape)\n",
    "\n",
    "# Информация о данных\n",
    "print(\"\\nИнформация о данных:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Первые 5 строк после очистки от некорректных символов:\n",
      "      id                                       comment_text  toxic  \\\n",
      "0  59848  this is so cool its like would you want your m...      0   \n",
      "1  59849  thank youии this would make my life a lot less...      0   \n",
      "2  59852  this is such an urgent design problem kudos to...      0   \n",
      "3  59855  is this something ill be able to install on my...      0   \n",
      "4  59856                haha you guys are a bunch of losers      1   \n",
      "\n",
      "   severe_toxic  obscene  threat  insult  identity_hate  \n",
      "0             0        0       0       0              0  \n",
      "1             0        0       0       0              0  \n",
      "2             0        0       0       0              0  \n",
      "3             0        0       0       0              0  \n",
      "4             0        0       0       1              0  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Функция для удаления некорректных символов из текста\n",
    "def remove_invalid_chars(text):\n",
    "    # Регулярное выражение для поиска и удаления некорректных символов\n",
    "    return re.sub(r'[^\\x00-\\x7Fа-яА-ЯёЁ]', '', text)\n",
    "\n",
    "# Применяем функцию к столбцу comment_text\n",
    "df['comment_text'] = df['comment_text'].apply(remove_invalid_chars)\n",
    "\n",
    "# Проверяем результат\n",
    "print(\"Первые 5 строк после очистки от некорректных символов:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Первые 5 комментариев для удаления (только спецсимволы/цифры):\n",
      "            id comment_text  toxic  severe_toxic  obscene  threat  insult  \\\n",
      "1957    243444          3 3      0             0        0       0       0   \n",
      "41939   292999           92      0             0        0       0       0   \n",
      "112134  379294            3      0             0        0       0       0   \n",
      "172795  453169            5      0             0        0       0       0   \n",
      "192565  476716           37      0             0        0       0       0   \n",
      "\n",
      "        identity_hate  \n",
      "1957                0  \n",
      "41939               0  \n",
      "112134              0  \n",
      "172795              0  \n",
      "192565              0  \n",
      "Данные после удаления комментариев с спецсимволами/цифрами: 3886304 строк\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Функция для проверки, состоит ли текст только из спецсимволов или цифр\n",
    "def is_special_or_numeric(text):\n",
    "    # Регулярное выражение для поиска символов, не являющихся буквами\n",
    "    return bool(re.match(r'^[\\W\\d_]+$', text))\n",
    "\n",
    "# Находим комментарии, состоящие только из спецсимволов или цифр\n",
    "special_or_numeric_comments = df[df['comment_text'].apply(is_special_or_numeric)]\n",
    "\n",
    "# Выводим первые 5 строк для удаления\n",
    "print(\"\\nПервые 5 комментариев для удаления (только спецсимволы/цифры):\")\n",
    "print(special_or_numeric_comments.head())\n",
    "\n",
    "# Удаляем такие комментарии\n",
    "df = df[~df['comment_text'].apply(is_special_or_numeric)]\n",
    "print(f\"Данные после удаления комментариев с спецсимволами/цифрами: {len(df)} строк\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Найдено комментариев короче 2 символов: 169\n",
      "              id comment_text  toxic  severe_toxic  obscene  threat  insult  \\\n",
      "276       240060                   0             0        0       0       0   \n",
      "316       240342                   0             0        0       0       0   \n",
      "8813      253136            k      0             0        0       0       0   \n",
      "10598     255298                   0             0        0       0       0   \n",
      "23946     271308            a      0             0        0       0       0   \n",
      "...          ...          ...    ...           ...      ...     ...     ...   \n",
      "3704260  6179076            а      0             0        0       0       0   \n",
      "3762787  6251575            и      0             0        0       0       0   \n",
      "3782988  6275289            о      0             0        0       0       0   \n",
      "3802465  6298600            и      0             0        0       0       0   \n",
      "3888971  7155424            o      0             0        0       0       0   \n",
      "\n",
      "         identity_hate  \n",
      "276                  0  \n",
      "316                  0  \n",
      "8813                 0  \n",
      "10598                0  \n",
      "23946                0  \n",
      "...                ...  \n",
      "3704260              0  \n",
      "3762787              0  \n",
      "3782988              0  \n",
      "3802465              0  \n",
      "3888971              0  \n",
      "\n",
      "[169 rows x 8 columns]\n",
      "Данные после удаления коротких комментариев: 3886135 строк\n"
     ]
    }
   ],
   "source": [
    "def find_short_comments(df, column='comment_text', min_length=2):\n",
    "    \"\"\"\n",
    "    Находит комментарии короче чем min_length символов.\n",
    "\n",
    "    Параметры:\n",
    "    df (pd.DataFrame): DataFrame с данными.\n",
    "    column (str): Название столбца с текстом комментариев.\n",
    "    min_length (int): Минимальная допустимая длина комментария.\n",
    "\n",
    "    Возвращает:\n",
    "    pd.DataFrame: DataFrame с комментариями короче min_length символов.\n",
    "    \"\"\"\n",
    "    # Находим комментарии короче min_length символов\n",
    "    short_comments = df[df[column].str.len() < min_length]\n",
    "    \n",
    "    return short_comments\n",
    "\n",
    "short_comments = find_short_comments(df, column='comment_text', min_length=2)\n",
    "\n",
    "# Вывод результата\n",
    "print(f\"Найдено комментариев короче 2 символов: {len(short_comments)}\")\n",
    "print(short_comments)\n",
    "\n",
    "df = df[df['comment_text'].str.len() >= 2]\n",
    "print(f\"Данные после удаления коротких комментариев: {len(df)} строк\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bokar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import pandas as pd\n",
    "from natasha import MorphVocab, Doc, Segmenter, NewsMorphTagger, NewsEmbedding\n",
    "\n",
    "# Загрузка стоп-слов для русского языка (используем NLTK)\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('russian'))\n",
    "\n",
    "# Инициализация компонентов Natasha\n",
    "segmenter = Segmenter()  # Для токенизации и сегментации текста\n",
    "emb = NewsEmbedding()  # Загрузка модели для морфологического анализа\n",
    "morph_tagger = NewsMorphTagger(emb)  # Для морфологического анализа\n",
    "morph_vocab = MorphVocab()  # Для лемматизации\n",
    "\n",
    "# Функция для предобработки текста\n",
    "def preprocess_text(text):\n",
    "    # Приведение к нижнему регистру\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Удаление пунктуации\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Удаление стоп-слов\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    \n",
    "    # Лемматизация с использованием Natasha\n",
    "    doc = Doc(text)\n",
    "    doc.segment(segmenter)  # Токенизация\n",
    "    doc.tag_morph(morph_tagger)  # Морфологический анализ\n",
    "    \n",
    "    # Извлечение лемм\n",
    "    for token in doc.tokens:\n",
    "        token.lemmatize(morph_vocab)  # Лемматизация каждого токена\n",
    "    lemmas = [token.lemma for token in doc.tokens]\n",
    "    text = ' '.join(lemmas)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Применение функции к столбцу с комментариями и замена исходного столбца\n",
    "df['comment_text'] = df['comment_text'].apply(preprocess_text)\n",
    "\n",
    "# Просмотр результата\n",
    "print(\"Первые 5 строк данных после предобработки:\")\n",
    "print(df[['comment_text']].head())\n",
    "\n",
    "# Удаление ненужных столбцов, если они есть\n",
    "df.drop(columns=['Unnamed: 0.1', 'Unnamed: 0'], inplace=True, errors='ignore')\n",
    "\n",
    "# 1. Просмотр первых нескольких строк данных\n",
    "print(\"\\nПервые 5 строк данных:\")\n",
    "print(df.head())\n",
    "\n",
    "# 2. Проверка размерности данных\n",
    "print(\"\\nРазмерность данных (строки, столбцы):\")\n",
    "print(df.shape)\n",
    "\n",
    "# 3. Типы данных и наличие пропущенных значений\n",
    "print(\"\\nИнформация о данных:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверка на дубликаты по столбцу comment_text\n",
    "duplicates_comment_text = df[df.duplicated('comment_text', keep=False)]\n",
    "print(f\"Найдено дубликатов по comment_text: {len(duplicates_comment_text)}\")\n",
    "\n",
    "# Удаление дубликатов по столбцу comment_text\n",
    "df = df.drop_duplicates(subset='comment_text', keep='first')\n",
    "print(f\"Данные после удаления дубликатов по comment_text: {len(df)} строк\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Изучите длину комментариев\n",
    "df['comment_length'] = df['comment_text'].apply(len)  # Добавляем столбец с длиной комментария\n",
    "\n",
    "# Построение гистограммы\n",
    "sns.histplot(df['comment_length'], bins=50)  # Строим гистограмму\n",
    "plt.title('Распределение длины комментариев')  # Добавляем заголовок\n",
    "plt.show()  # Отображаем график\n",
    "\n",
    "# Печать обобщенных результатов в консоль\n",
    "print(\"Обобщенные результаты анализа длины комментариев:\")\n",
    "print(df['comment_length'].describe())  # Основные статистические метрики\n",
    "print(f\"\\nМинимальная длина комментария: {df['comment_length'].min()}\")  # Минимальная длина\n",
    "print(f\"Максимальная длина комментария: {df['comment_length'].max()}\")  # Максимальная длина\n",
    "print(f\"Средняя длина комментария: {df['comment_length'].mean():.2f}\")  # Средняя длина\n",
    "print(f\"Медианная длина комментария: {df['comment_length'].median()}\")  # Медиана\n",
    "\n",
    "# Удаляем столбец comment_length после построения графика и анализа\n",
    "df.drop(columns=['comment_length'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Список меток\n",
    "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "# Добавляем столбец с количеством меток на один комментарий\n",
    "df['num_labels'] = df[labels].sum(axis=1)\n",
    "\n",
    "# Построение графика\n",
    "sns.countplot(x='num_labels', data=df)\n",
    "plt.title('Количество меток на один комментарий')\n",
    "plt.show()\n",
    "\n",
    "# Печать данных в консоль\n",
    "print(\"Распределение количества меток на один комментарий:\")\n",
    "print(df['num_labels'].value_counts().sort_index())  # Сортировка по количеству меток\n",
    "\n",
    "# Дополнительная информация\n",
    "print(\"\\nДополнительная информация:\")\n",
    "print(f\"Общее количество комментариев: {len(df)}\")\n",
    "print(f\"Комментарии без меток: {len(df[df['num_labels'] == 0])} ({(len(df[df['num_labels'] == 0]) / len(df)) * 100:.2f}%)\")\n",
    "print(f\"Комментарии с хотя бы одной меткой: {len(df[df['num_labels'] > 0])} ({(len(df[df['num_labels'] > 0]) / len(df)) * 100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Создаем новый столбец, который указывает, является ли комментарий токсичным (хотя бы одна метка = 1)\n",
    "df['any_toxic'] = df[labels].max(axis=1)\n",
    "\n",
    "# Распределение для суммарного столбца\n",
    "print(\"Распределение для любого типа токсичности (any_toxic):\")\n",
    "print(df['any_toxic'].value_counts(normalize=True))  # Процентное соотношение\n",
    "\n",
    "# Дополнительная информация\n",
    "print(\"\\nДополнительная информация:\")\n",
    "print(f\"Общее количество комментариев: {len(df)}\")\n",
    "print(f\"Количество нетоксичных комментариев: {len(df[df['any_toxic'] == 0])} ({(len(df[df['any_toxic'] == 0]) / len(df)) * 100:.1f}%)\")\n",
    "print(f\"Количество токсичных комментариев: {len(df[df['any_toxic'] == 1])} ({(len(df[df['any_toxic'] == 1]) / len(df)) * 100:.1f}%)\")\n",
    "\n",
    "# Визуализация с процентами на столбиках\n",
    "plt.figure(figsize=(8, 6))\n",
    "ax = sns.countplot(x='any_toxic', data=df)\n",
    "plt.title('Распределение для любого типа токсичности (any_toxic)')\n",
    "\n",
    "# Добавление процентных значений на столбики\n",
    "for p in ax.patches:\n",
    "    height = p.get_height()  # Высота столбика\n",
    "    percentage = (height / len(df)) * 100  # Процентное соотношение\n",
    "    ax.text(\n",
    "        p.get_x() + p.get_width() / 2.,  # Позиция по X (центр столбика)\n",
    "        height + 0.1,                    # Позиция по Y (немного выше столбика)\n",
    "        f'{percentage:.1f}%',            # Текст с процентом\n",
    "        ha='center'                      # Выравнивание по центру\n",
    "    )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# Функция для построения облака слов\n",
    "def generate_wordcloud(text, title):\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Тексты для токсичных и нетоксичных комментариев\n",
    "toxic_text = ' '.join(df[df['toxic'] == 1]['comment_text'])\n",
    "non_toxic_text = ' '.join(df[df['toxic'] == 0]['comment_text'])\n",
    "\n",
    "# Дополнительная информация\n",
    "print(\"Дополнительная информация по токсичным комментариям:\")\n",
    "print(f\"Количество токсичных комментариев: {len(df[df['toxic'] == 1])} ({(len(df[df['toxic'] == 1]) / len(df)) * 100:.1f}%)\")\n",
    "print(\"Топ-10 самых частых слов в токсичных комментариях:\")\n",
    "toxic_words = toxic_text.split()\n",
    "toxic_word_counts = Counter(toxic_words)\n",
    "print(toxic_word_counts.most_common(10))\n",
    "\n",
    "print(\"\\nДополнительная информация по нетоксичным комментариям:\")\n",
    "print(f\"Количество нетоксичных комментариев: {len(df[df['toxic'] == 0])} ({(len(df[df['toxic'] == 0]) / len(df)) * 100:.1f}%)\")\n",
    "print(\"Топ-10 самых частых слов в нетоксичных комментариях:\")\n",
    "non_toxic_words = non_toxic_text.split()\n",
    "non_toxic_word_counts = Counter(non_toxic_words)\n",
    "print(non_toxic_word_counts.most_common(10))\n",
    "\n",
    "# Облако слов для токсичных комментариев\n",
    "generate_wordcloud(toxic_text, 'Облако слов для токсичных комментариев')\n",
    "\n",
    "# Облако слов для нетоксичных комментариев\n",
    "generate_wordcloud(non_toxic_text, 'Облако слов для нетоксичных комментариев')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Список меток\n",
    "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "# Подсчет количества комментариев для каждой метки\n",
    "label_counts = df[labels].sum()\n",
    "\n",
    "# Расчет процентного соотношения для каждой метки\n",
    "label_percentages = df[labels].mean() * 100\n",
    "\n",
    "# Создание DataFrame для вывода\n",
    "result_df = pd.DataFrame({\n",
    "    'Метка': label_counts.index,\n",
    "    'Количество': label_counts.values,\n",
    "    'Процент (%)': label_percentages.values\n",
    "})\n",
    "\n",
    "# Визуализация в виде bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.barplot(x=label_counts.index, y=label_counts.values, palette='viridis')\n",
    "plt.title('Распределение меток')\n",
    "plt.ylabel('Количество комментариев')\n",
    "plt.xlabel('Метки')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Добавление процентного соотношения наверху каждого столбца\n",
    "for p in ax.patches:\n",
    "    height = p.get_height()  # Высота столбца\n",
    "    ax.text(\n",
    "        p.get_x() + p.get_width() / 2.,  # Позиция по X (центр столбца)\n",
    "        height + 0.1,                    # Позиция по Y (немного выше столбца)\n",
    "        f'{height / len(df) * 100:.1f}%',  # Текст с процентом\n",
    "        ha='center'                      # Выравнивание по центру\n",
    "    )\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Вывод результата в консоль\n",
    "print(\"Результаты анализа меток:\")\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, inspect\n",
    "\n",
    "# Подключение к базе данных\n",
    "engine = create_engine(\"sqlite:///cleaned_comments.db\")\n",
    "\n",
    "# Вывод содержимого таблицы\n",
    "def print_table_contents(engine, table_name):\n",
    "    # Загрузка данных из таблицы в DataFrame\n",
    "    df = pd.read_sql_table(table_name, engine)\n",
    "    \n",
    "    # Вывод первых 5 строк\n",
    "    print(f\"Первые 5 строк таблицы '{table_name}':\")\n",
    "    print(df.head())\n",
    "    \n",
    "    # Вывод количества записей\n",
    "    print(f\"\\nКоличество записей в таблице '{table_name}': {len(df)}\")\n",
    "    \n",
    "    # Вывод структуры таблицы (столбцы и их типы)\n",
    "    inspector = inspect(engine)\n",
    "    columns = inspector.get_columns(table_name)\n",
    "    print(\"\\nСтруктура таблицы:\")\n",
    "    for column in columns:\n",
    "        print(f\"Столбец: {column['name']}, Тип: {column['type']}\")\n",
    "\n",
    "# Запись DataFrame в базу данных\n",
    "table_name = \"cleaned_comments\"\n",
    "df.to_sql(table_name, engine, if_exists=\"replace\", index=False)\n",
    "print(f\"\\nДанные успешно записаны в таблицу '{table_name}'.\")\n",
    "\n",
    "\n",
    "# Проверка наличия таблицы\n",
    "inspector = inspect(engine)\n",
    "tables = inspector.get_table_names()\n",
    "print(f\"Таблицы в базе данных: {tables}\")\n",
    "\n",
    "if \"cleaned_comments\" in tables:\n",
    "    # Вывод содержимого таблицы\n",
    "    def print_table_contents(engine, table_name):\n",
    "        # Загрузка данных из таблицы в DataFrame\n",
    "        df = pd.read_sql_table(table_name, engine)\n",
    "        \n",
    "        # Вывод первых 5 строк\n",
    "        print(f\"Первые 5 строк таблицы '{table_name}':\")\n",
    "        print(df.head())\n",
    "        \n",
    "        # Вывод количества записей\n",
    "        print(f\"\\nКоличество записей в таблице '{table_name}': {len(df)}\")\n",
    "        \n",
    "        # Вывод структуры таблицы (столбцы и их типы)\n",
    "        columns = inspector.get_columns(table_name)\n",
    "        print(\"\\nСтруктура таблицы:\")\n",
    "        for column in columns:\n",
    "            print(f\"Столбец: {column['name']}, Тип: {column['type']}\")\n",
    "\n",
    "    # Вывод содержимого таблицы и дополнительной информации\n",
    "    print_table_contents(engine, \"cleaned_comments\")\n",
    "else:\n",
    "    print(\"Таблица 'cleaned_comments' не найдена в базе данных.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
